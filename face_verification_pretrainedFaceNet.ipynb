{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_first')\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from fr_utils import *\n",
    "from inception_blocks_v2 import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FRmodel = faceRecoModel(input_shape=(3, 96, 96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 3743280\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Params:\", FRmodel.count_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# triplet_loss\n",
    "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    pos_dist = tf.reduce_sum((tf.square(tf.subtract(anchor,positive))))\n",
    "    neg_dist = tf.reduce_sum((tf.square(tf.subtract(anchor,negative))))\n",
    "    basic_loss = tf.add(tf.subtract(pos_dist,neg_dist),alpha)\n",
    "    loss = tf.reduce_sum(tf.maximum(basic_loss, 0.0))    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 350.026\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as test:\n",
    "    tf.set_random_seed(1)\n",
    "    y_true = (None, None, None)\n",
    "    y_pred = (tf.random_normal([3, 128], mean=6, stddev=0.1, seed = 1),\n",
    "              tf.random_normal([3, 128], mean=1, stddev=1, seed = 1),\n",
    "              tf.random_normal([3, 128], mean=3, stddev=4, seed = 1))\n",
    "    loss = triplet_loss(y_true, y_pred)\n",
    "    \n",
    "    print(\"loss = \" + str(loss.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FRmodel.compile(optimizer = 'adam', loss = triplet_loss, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_weights_from_FaceNet(FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "database = {}\n",
    "database[\"danielle\"] = img_to_encoding(\"images/danielle.png\", FRmodel)\n",
    "database[\"younes\"] = img_to_encoding(\"images/younes.jpg\", FRmodel)\n",
    "database[\"tian\"] = img_to_encoding(\"images/tian.jpg\", FRmodel)\n",
    "database[\"andrew\"] = img_to_encoding(\"images/andrew.jpg\", FRmodel)\n",
    "database[\"kian\"] = img_to_encoding(\"images/kian.jpg\", FRmodel)\n",
    "database[\"dan\"] = img_to_encoding(\"images/dan.jpg\", FRmodel)\n",
    "database[\"sebastiano\"] = img_to_encoding(\"images/sebastiano.jpg\", FRmodel)\n",
    "database[\"bertrand\"] = img_to_encoding(\"images/bertrand.jpg\", FRmodel)\n",
    "database[\"kevin\"] = img_to_encoding(\"images/kevin.jpg\", FRmodel)\n",
    "database[\"felix\"] = img_to_encoding(\"images/felix.jpg\", FRmodel)\n",
    "database[\"benoit\"] = img_to_encoding(\"images/benoit.jpg\", FRmodel)\n",
    "database[\"arnaud\"] = img_to_encoding(\"images/arnaud.jpg\", FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Image data can not convert to float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-541180bfd34c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"images/arnaud.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python3.5/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, hold, data, **kwargs)\u001b[0m\n\u001b[1;32m   3156\u001b[0m                         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3157\u001b[0m                         \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3158\u001b[0;31m                         **kwargs)\n\u001b[0m\u001b[1;32m   3159\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3160\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python3.5/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1890\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1891\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1892\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1893\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1894\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python3.5/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5116\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5118\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5119\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/tensorflow/lib/python3.5/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    543\u001b[0m         if (self._A.dtype != np.uint8 and\n\u001b[1;32m    544\u001b[0m                 not np.can_cast(self._A.dtype, np.float)):\n\u001b[0;32m--> 545\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Image data can not convert to float\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         if (self._A.ndim not in (2, 3) or\n",
      "\u001b[0;31mTypeError\u001b[0m: Image data can not convert to float"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njjp5sn4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12cfa56a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "img=mpimg.imread('your_image.png')\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()\n",
    "plt.imshow(\"images/arnaud.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def verify(image_path, identity, database, model):\n",
    "    encoding = img_to_encoding(image_path, model)\n",
    "    dist = np.linalg.norm((encoding - database[identity]), ord = 2)\n",
    "    if dist < 0.7:\n",
    "        print(\"It's \" + str(identity) + \", welcome home!\")\n",
    "        door_open = True\n",
    "    else:\n",
    "        print(\"It's not \" + str(identity) + \", please go away\")\n",
    "        door_open = False        \n",
    "    return dist, door_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's younes, welcome home!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.1915442, True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify(\"images/camera_0.jpg\", \"younes\", database, FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's kian, welcome home!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.25956395, True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify(\"images/camera_2.jpg\", \"kian\", database, FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def who_is_it(image_path, database, model):\n",
    "    encoding = img_to_encoding(image_path, model)\n",
    "    min_dist = 100\n",
    "    for (name, db_enc) in database.items():\n",
    "        dist = np.linalg.norm((encoding - db_enc), ord = 2)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            identity = name    \n",
    "    if min_dist > 0.7:\n",
    "        print(\"Not in the database.\")\n",
    "    else:\n",
    "        print (\"it's \" + str(identity) + \", the distance is \" + str(min_dist))\n",
    "        \n",
    "    return min_dist, identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it's younes, the distance is 0.191544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.1915442, 'younes')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "who_is_it(\"images/camera_0.jpg\", database, FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
